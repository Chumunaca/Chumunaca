{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "A100",
      "mount_file_id": "14FlrkNBDUT0Vozwfg9gJZwydZ8P_ANzs",
      "authorship_tag": "ABX9TyOLILOoXCqnHvmkLsiHJlus",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chumunaca/Chumunaca/blob/main/DETECTRON.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BEljIGemt9AK"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 1. INSTALACI√ìN DE DEPENDENCIAS\n",
        "# ==========================================\n",
        "print(\"üöÄ --- FASE 1: INSTALANDO DETECTRON2 ---\")\n",
        "# Removed explicit pyyaml==5.1 installation as it failed and detectron2 from source will handle its own pyyaml dependencies.\n",
        "\n",
        "import torch\n",
        "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
        "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
        "print(f\"   -> Detectado: PyTorch {TORCH_VERSION} | CUDA {CUDA_VERSION}\")\n",
        "\n",
        "# Instalaci√≥n compatible con Colab\n",
        "# Reemplazar la instalaci√≥n basada en wheels con la instalaci√≥n desde el repositorio de GitHub.\n",
        "# Esto es m√°s robusto en Colab ya que construye Detectron2 contra las versiones existentes de PyTorch/CUDA.\n",
        "print(f\"   -> Instalando Detectron2 desde el c√≥digo fuente (para compatibilidad con PyTorch {TORCH_VERSION} | CUDA {CUDA_VERSION})...\")\n",
        "!pip install -q 'git+https://github.com/facebookresearch/detectron2.git'\n",
        "\n",
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger()\n",
        "print(\"‚úÖ Detectron2 instalado.\")\n",
        "\n",
        "# ==========================================\n",
        "# 2. DESCOMPRESI√ìN DE DATOS (AL DISCO R√ÅPIDO)\n",
        "# ==========================================\n",
        "import os\n",
        "\n",
        "print(\"\\nüöÄ --- FASE 2: MOVIMIENTO DE DATOS ---\")\n",
        "# Rutas en Drive\n",
        "ZIP_PATH_DRIVE = '/content/drive/MyDrive/TESIS/imagenes/vindr.zip'\n",
        "CSV_PATH_DRIVE = '/content/drive/MyDrive/TESIS/TESIS/finding_annotations.csv'\n",
        "\n",
        "# Ruta Local (R√°pida)\n",
        "EXTRACT_DIR = '/content/vindr_data'\n",
        "IMG_BASE_DIR = os.path.join(EXTRACT_DIR, 'images_png')\n",
        "\n",
        "if not os.path.exists(IMG_BASE_DIR):\n",
        "    print(f\"   -> Descomprimiendo {ZIP_PATH_DRIVE} en local...\")\n",
        "    os.makedirs(EXTRACT_DIR, exist_ok=True)\n",
        "    # Unzip r√°pido\n",
        "    !unzip -q \"{ZIP_PATH_DRIVE}\" -d \"{EXTRACT_DIR}\"\n",
        "    print(\"‚úÖ Descompresi√≥n terminada. Im√°genes listas en local.\")\n",
        "else:\n",
        "    print(\"‚úÖ Las im√°genes ya estaban en el disco local.\")\n",
        "\n",
        "# ==========================================\n",
        "# 3. PREPARACI√ìN Y ESCALADO (La Regla de 3)\n",
        "# ==========================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from detectron2.structures import BoxMode\n",
        "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
        "from ast import literal_eval\n",
        "from tqdm import tqdm\n",
        "\n",
        "print(\"\\nüöÄ --- FASE 3: REGISTRO DE DATASET ---\")\n",
        "\n",
        "# TUS DIMENSIONES OBJETIVO\n",
        "TARGET_H = 1520\n",
        "TARGET_W = 912\n",
        "\n",
        "# Cargar CSV\n",
        "df = pd.read_csv(CSV_PATH_DRIVE)\n",
        "df = df[df['xmin'].notna()].copy() # Solo lesiones\n",
        "\n",
        "# Limpiar categor√≠as\n",
        "def clean_cat(x):\n",
        "    try:\n",
        "        if isinstance(x, str):\n",
        "            l = literal_eval(x)\n",
        "            return l[0] if l else \"Unknown\"\n",
        "    except:\n",
        "        return x\n",
        "    return x\n",
        "\n",
        "df['category'] = df['finding_categories'].apply(clean_cat)\n",
        "classes = sorted(df['category'].unique())\n",
        "class_to_id = {name: i for i, name in enumerate(classes)}\n",
        "print(f\"   -> Clases encontradas: {classes}\")\n",
        "\n",
        "def get_vindr_dicts(split):\n",
        "    dataset_dicts = []\n",
        "    data_split = df[df['split'] == split]\n",
        "    grouped = data_split.groupby('image_id')\n",
        "\n",
        "    print(f\"   -> Procesando split: {split}...\")\n",
        "    for img_id, group in tqdm(grouped):\n",
        "        study_id = group.iloc[0]['study_id']\n",
        "        # Ruta local r√°pida\n",
        "        file_path = os.path.join(IMG_BASE_DIR, study_id, f\"{img_id}.png\")\n",
        "\n",
        "        if not os.path.exists(file_path): continue\n",
        "\n",
        "        # Dimensiones ORIGINALES (del CSV)\n",
        "        orig_h = int(group.iloc[0]['height'])\n",
        "        orig_w = int(group.iloc[0]['width'])\n",
        "\n",
        "        # Factores de escala\n",
        "        scale_x = TARGET_W / orig_w\n",
        "        scale_y = TARGET_H / orig_h\n",
        "\n",
        "        record = {}\n",
        "        record[\"file_name\"] = file_path\n",
        "        record[\"image_id\"] = img_id\n",
        "        record[\"height\"] = TARGET_H\n",
        "        record[\"width\"] = TARGET_W\n",
        "\n",
        "        objs = []\n",
        "        for _, row in group.iterrows():\n",
        "            cat = row['category']\n",
        "            if cat not in class_to_id: continue\n",
        "\n",
        "            # ESCALADO DE CAJAS\n",
        "            x1 = row['xmin'] * scale_x\n",
        "            y1 = row['ymin'] * scale_y\n",
        "            x2 = row['xmax'] * scale_x\n",
        "            y2 = row['ymax'] * scale_y\n",
        "\n",
        "            obj = {\n",
        "                \"bbox\": [x1, y1, x2, y2],\n",
        "                \"bbox_mode\": BoxMode.XYXY_ABS,\n",
        "                \"category_id\": class_to_id[cat],\n",
        "            }\n",
        "            objs.append(obj)\n",
        "\n",
        "        record[\"annotations\"] = objs\n",
        "        dataset_dicts.append(record)\n",
        "    return dataset_dicts\n",
        "\n",
        "# Registrar\n",
        "DatasetCatalog.clear()\n",
        "MetadataCatalog.clear()\n",
        "for d in [\"training\", \"test\"]:\n",
        "    DatasetCatalog.register(f\"vindr_{d}\", lambda x=d: get_vindr_dicts(x))\n",
        "    MetadataCatalog.get(f\"vindr_{d}\").set(thing_classes=classes)\n",
        "\n",
        "print(\"‚úÖ Dataset registrado y escalado correctamente.\")"
      ],
      "metadata": {
        "id": "xpLk030UDGzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "\n",
        "# Agarrar la metadata de lo que acabamos de registrar\n",
        "vindr_metadata = MetadataCatalog.get(\"vindr_training\")\n",
        "\n",
        "#Traer los diccionarios (la lista de im√°genes y cajas)\n",
        "dataset_dicts = get_vindr_dicts(\"training\")\n",
        "\n",
        "print(\"üé≤ Agarrando 3 im√°genes al azar para ver si las cajas coinciden con las lesiones...\")\n",
        "\n",
        "# Visualizar 3 muestras\n",
        "for d in random.sample(dataset_dicts, 3):\n",
        "    # Leer imagen\n",
        "    img = cv2.imread(d[\"file_name\"])\n",
        "\n",
        "    # Dibujar las cajas \"Ground Truth\" (La Verdad Absoluta)\n",
        "    visualizer = Visualizer(img[:, :, ::-1], metadata=vindr_metadata, scale=0.5)\n",
        "    out = visualizer.draw_dataset_dict(d)\n",
        "\n",
        "    # Mostrar con Matplotlib\n",
        "    plt.figure(figsize=(10, 12))\n",
        "    plt.imshow(out.get_image())\n",
        "    plt.title(f\"Imagen: {d['image_id']}\\nCajas Reales (Entrenamiento)\")\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "WCEksu9AGhil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from detectron2.engine import DefaultTrainer\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2 import model_zoo\n",
        "\n",
        "# --- 1. LIMPIEZA (Borr√≥n y Cuenta Nueva) ---\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/TESIS/detectron_output_vindr_A100_HEAVY\"\n",
        "\n",
        "if os.path.exists(OUTPUT_DIR):\n",
        "    print(f\"üóëÔ∏è Borrando modelo anterior en {OUTPUT_DIR}...\")\n",
        "    shutil.rmtree(OUTPUT_DIR) # Adi√≥s basura\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "print(f\"‚ú® Directorio limpio y listo: {OUTPUT_DIR}\")\n",
        "\n",
        "# --- 2. CONFIGURAR LA BESTIA (X101) ---\n",
        "print(\"‚öôÔ∏è Cargando configuraci√≥n de ResNeXt-101 (The Beast)...\")\n",
        "cfg = get_cfg()\n",
        "\n",
        "# Usamos X_101_32x8d_FPN_3x (Mucho m√°s poderoso que R_50)\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x.yaml\"))\n",
        "\n",
        "# Datos\n",
        "cfg.DATASETS.TRAIN = (\"vindr_training\",)\n",
        "cfg.DATASETS.TEST = ()\n",
        "cfg.DATALOADER.NUM_WORKERS = 4\n",
        "\n",
        "# Pesos Iniciales\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x.yaml\")\n",
        "\n",
        "# --- 3. HIPERPAR√ÅMETROS AGRESIVOS PARA A100 ---\n",
        "# X101 consume m√°s VRAM, as√≠ que ajustamos el batch\n",
        "cfg.SOLVER.IMS_PER_BATCH = 6   # 6 im√°genes gigantes por batch (A100 aguanta)\n",
        "cfg.SOLVER.BASE_LR = 0.001     # Learning Rate\n",
        "cfg.SOLVER.WARMUP_ITERS = 1000 # Calentamiento suave para no romper gradientes\n",
        "\n",
        "# ¬°AQUI EST√Å EL CAMBIO FUERTE! -> M√ÅS TIEMPO\n",
        "# Si 2000 iters fueron 15 min...\n",
        "# 10,000 iters ser√°n como 1 hora y cachito. ¬°DALE SIN MIEDO!\n",
        "cfg.SOLVER.MAX_ITER = 10000\n",
        "\n",
        "cfg.SOLVER.STEPS = (7000, 9000) # Bajamos el LR al final para pulir\n",
        "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 512 # M√°s muestras por imagen (m√°s fino)\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = len(classes) # Tus clases\n",
        "\n",
        "# Umbral de prueba (para que cuando termine, guarde esto en el config)\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.2\n",
        "\n",
        "cfg.OUTPUT_DIR = OUTPUT_DIR\n",
        "\n",
        "# --- 4. ENTRENAR ---\n",
        "print(f\"üî• ARRANCANDO LA A100 CON X101 POR {cfg.SOLVER.MAX_ITER} ITERACIONES...\")\n",
        "print(\"Ve por cenar, echarte un sue√±ito o lo que quieras. Esto va a quedar perr√≥n.\")\n",
        "\n",
        "trainer = DefaultTrainer(cfg)\n",
        "trainer.resume_or_load(resume=False)\n",
        "trainer.train()\n",
        "\n",
        "print(f\"‚úÖ ¬°TERMIN√ì LA BESTIA! Modelo guardado en: {cfg.OUTPUT_DIR}\")"
      ],
      "metadata": {
        "id": "YIZ1TJQ3MD4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from detectron2.utils.visualizer import ColorMode\n",
        "import random\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from detectron2.engine import DefaultPredictor\n",
        "\n",
        "print(\"--- üîÆ VISUALIZANDO LA BESTIA (X101) ---\")\n",
        "\n",
        "# 1. Configurar para Inferencia\n",
        "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.15 # Umbral de confianza (40%)\n",
        "# Si no sale nada con 0.4, b√°jalo a 0.2\n",
        "cfg.DATASETS.TEST = (\"vindr_test\", )\n",
        "\n",
        "predictor = DefaultPredictor(cfg)\n",
        "\n",
        "# 2. Visualizar\n",
        "dataset_dicts = get_vindr_dicts(\"test\")\n",
        "\n",
        "# Vamos a ver 5 im√°genes al azar\n",
        "for d in random.sample(dataset_dicts, 5):\n",
        "    im = cv2.imread(d[\"file_name\"])\n",
        "    outputs = predictor(im)\n",
        "\n",
        "    # Dibujar predicciones\n",
        "    v = Visualizer(im[:, :, ::-1],\n",
        "                   metadata=MetadataCatalog.get(\"vindr_train\"),\n",
        "                   scale=0.5,\n",
        "                   instance_mode=ColorMode.IMAGE_BW # B/N para contraste\n",
        "    )\n",
        "    out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "\n",
        "    plt.figure(figsize=(10, 12))\n",
        "    plt.imshow(out.get_image())\n",
        "    plt.title(f\"Predicci√≥n X101 | ID: {d['image_id']}\")\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "-Io3YfEinXOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from datetime import datetime\n",
        "\n",
        "# Ruta donde se guard√≥ el entrenamiento\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/TESIS/detectron_output_vindr_A100_HEAVY\"\n",
        "MODEL_PATH = os.path.join(OUTPUT_DIR, \"model_final.pth\")\n",
        "\n",
        "if os.path.exists(MODEL_PATH):\n",
        "    print(f\"‚úÖ ¬°CONFIRMADO! El modelo existe en: {MODEL_PATH}\")\n",
        "\n",
        "    # Hacemos una copia de seguridad con fecha y hora (por si reentrenas por error y lo borras)\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
        "    backup_path = os.path.join(OUTPUT_DIR, f\"model_final_BACKUP_{timestamp}.pth\")\n",
        "\n",
        "    shutil.copy(MODEL_PATH, backup_path)\n",
        "    print(f\"üîí Copia de seguridad creada: {backup_path}\")\n",
        "    print(\"¬°Ya nadie te lo borra!\")\n",
        "else:\n",
        "    print(\"üö® ¬°ALERTA! No encuentro el modelo. ¬øTermin√≥ de entrenar?\")"
      ],
      "metadata": {
        "id": "0RZm_6bCn2q5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
        "from detectron2.data import build_detection_test_loader\n",
        "\n",
        "# --- 1. GR√ÅFICA DE APRENDIZAJE (LOSS CURVE) ---\n",
        "# Detectron guarda todo en metrics.json\n",
        "json_file = os.path.join(cfg.OUTPUT_DIR, \"metrics.json\")\n",
        "\n",
        "if os.path.exists(json_file):\n",
        "    data = []\n",
        "    with open(json_file) as f:\n",
        "        for line in f:\n",
        "            data.append(json.loads(line))\n",
        "\n",
        "    df_metrics = pd.DataFrame(data)\n",
        "\n",
        "    # Plotear\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(df_metrics['iteration'], df_metrics['total_loss'], label='Error Total (Loss)', color='red')\n",
        "    if 'loss_cls' in df_metrics.columns:\n",
        "        plt.plot(df_metrics['iteration'], df_metrics['loss_cls'], label='Error Clasificaci√≥n', color='blue', alpha=0.5)\n",
        "    if 'loss_box_reg' in df_metrics.columns:\n",
        "        plt.plot(df_metrics['iteration'], df_metrics['loss_box_reg'], label='Error Coordenadas', color='green', alpha=0.5)\n",
        "\n",
        "    plt.title('Curva de Aprendizaje: Faster R-CNN X101', fontsize=14, fontweight='bold')\n",
        "    plt.xlabel('Iteraciones')\n",
        "    plt.ylabel('Loss (Menos es mejor)')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.savefig('grafica_loss_detectron.png') # Se guarda para que la descargues\n",
        "    plt.show()\n",
        "    print(\"‚úÖ Gr√°fica de Loss generada.\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No encontr√© el archivo metrics.json (quiz√°s borraste la carpeta output?)\")\n",
        "\n",
        "# --- 2. EVALUACI√ìN COCO (EST√ÅNDAR DE ORO) ---\n",
        "print(\"\\nüî• Corriendo Evaluaci√≥n Estad√≠stica COCO (Esto tarda unos minutos)...\")\n",
        "\n",
        "# Configurar evaluador\n",
        "evaluator = COCOEvaluator(\"vindr_test\", output_dir=cfg.OUTPUT_DIR)\n",
        "val_loader = build_detection_test_loader(cfg, \"vindr_test\")\n",
        "\n",
        "# Correr inferencia masiva\n",
        "print(\"Calculando precisi√≥n por clase...\")\n",
        "results = inference_on_dataset(predictor.model, val_loader, evaluator)\n",
        "\n",
        "# Imprimir bonito\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"RESULTADOS ESTAD√çSTICOS FINALES (mAP)\")\n",
        "print(\"=\"*50)\n",
        "# Esto imprime la tabla desglosada por clase (AP50, AP75, etc.)\n",
        "print(results)"
      ],
      "metadata": {
        "id": "2FbzVPi9n4WD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}